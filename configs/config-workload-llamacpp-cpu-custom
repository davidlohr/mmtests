export MMTESTS="llamacpp"

# List of monitors
export RUN_MONITOR=yes
export MONITORS_ALWAYS=
export MONITORS_GZIP="proc-vmstat mpstat"
export MONITORS_WITH_LATENCY="vmstat"
export MONITOR_UPDATE_FREQUENCY=10

# LLama.cpp benchmark - Custom model example with OPT-125M
export LLAMACPP_COMPUTE_BACKEND=cpu                      # Use CPU backend for inference
export LLAMACPP_MIN_THREADS=8                            # Start thread scaling from 1 thread
export LLAMACPP_MAX_THREADS=$NUMCPUS                     # Scale up to all available CPU cores
export LLAMACPP_BATCH_SIZE=256                           # Batch size optimized for smaller model
export LLAMACPP_ITERATIONS=2                             # Number of benchmark iterations to run
export LLAMACPP_PROMPT_TOKENS=256                        # Number of prompt tokens to process
export LLAMACPP_GEN_TOKENS=64                            # Number of tokens to generate

# Custom model specification - Qwen2-0.5B Q4_K_M (~350MB)
# This overrides automatic model selection based on memory size
export LLAMACPP_MODEL_URL="https://huggingface.co/Qwen/Qwen2-0.5B-Instruct-GGUF/resolve/main/qwen2-0_5b-instruct-q4_k_m.gguf"

# Advanced options (optional)
#export LLAMACPP_MODEL_NUMA=distribute                   # NUMA memory distribution strategy
#export LLAMACPP_WARMUP=no                               # Disable warmup for faster testing
#export LLAMACPP_NO_KVOFFLOAD=1                          # Disable KV cache offloading
