#!/bin/bash
###SHELLPACK preamble llamacpp-bench 0


###SHELLPACK parseargBegin
###SHELLPACK parseargParam --model-size		LLAMACPP_MODEL_SIZE
###SHELLPACK parseargParam --model-url		LLAMACPP_MODEL_URL
###SHELLPACK parseargParam --compute-backend	LLAMACPP_COMPUTE_BACKEND
###SHELLPACK parseargParam --min-threads	LLAMACPP_MIN_THREADS
###SHELLPACK parseargParam --max-threads	LLAMACPP_MAX_THREADS
###SHELLPACK parseargParam --batch-size		LLAMACPP_BATCH_SIZE
###SHELLPACK parseargParam --iterations		LLAMACPP_ITERATIONS
###SHELLPACK parseargParam --prompt-tokens	LLAMACPP_PROMPT_TOKENS
###SHELLPACK parseargParam --gen-tokens		LLAMACPP_GEN_TOKENS
###SHELLPACK parseargParam --numa		LLAMACPP_MODEL_NUMA
###SHELLPACK parseargParam --warmup		LLAMACPP_WARMUP
###SHELLPACK parseargParam --no-kv-offload	LLAMACPP_NO_KVOFFLOAD
###SHELLPACK parseargEnd
###SHELLPACK monitor_hooks

###SHELLPACK check_install_required llamacpp-0
###SHELLPACK init_complete

LLAMACPP_INSTALL_PATH=$SHELLPACK_SOURCES/llamacpp-${VERSION}-installed
MODEL_DIR=$LLAMACPP_INSTALL_PATH/models

# Function to select model based on memory bytes
select_model_by_memory_bytes() {
	local allocated_memory_bytes=$1
	local allocated_memory_mb=$((allocated_memory_bytes / 1024 / 1024))
	
	echo "System memory: $((MEMTOTAL_BYTES/1024/1024))MB, Allocated for model: ${allocated_memory_mb}MB"
	
	# Select model based on allocated memory (using realistic thresholds for large systems)
	# Conservative model selection to avoid 401 errors, but with proper scaling for large memory
	if [ $allocated_memory_mb -ge 300000 ]; then
		# 300GB+: Large 7B model for massive memory systems
		MODEL_NAME="mistral-7b-instruct-v0.1.Q4_K_M.gguf"
		MODEL_URL="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf"
		MODEL_DESC="Mistral 7B Instruct Q4_K_M (4.4GB model) - Massive memory config (300GB+)"
	elif [ $allocated_memory_mb -ge 100000 ]; then
		# 100GB+: 7B model for very large memory systems
		MODEL_NAME="mistral-7b-instruct-v0.1.Q4_K_M.gguf"
		MODEL_URL="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf"
		MODEL_DESC="Mistral 7B Instruct Q4_K_M (4.4GB model) - Very large memory config (100GB+)"
	elif [ $allocated_memory_mb -ge 50000 ]; then
		# 50GB+: 7B model for large memory systems
		MODEL_NAME="capybarahermes-2.5-mistral-7b.Q4_K_M.gguf"
		MODEL_URL="https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF/resolve/main/capybarahermes-2.5-mistral-7b.Q4_K_M.gguf"
		MODEL_DESC="CapybaraHermes 2.5 Mistral 7B Q4_K_M (4.4GB model) - Large memory config (50GB+)"
	elif [ $allocated_memory_mb -ge 25000 ]; then
		# 25GB+: 7B model for medium-large memory systems  
		MODEL_NAME="mistral-7b-instruct-v0.1.Q4_K_M.gguf"
		MODEL_URL="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf"
		MODEL_DESC="Mistral 7B Instruct Q4_K_M (4.4GB model) - Medium-large memory config (25GB+)"
	elif [ $allocated_memory_mb -ge 8000 ]; then
		# 8GB+: 7B model for medium memory systems
		MODEL_NAME="mistral-7b-instruct-v0.1.Q4_K_M.gguf"
		MODEL_URL="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf"
		MODEL_DESC="Mistral 7B Instruct Q4_K_M (4.4GB model) - Medium memory config (8GB+)"
	elif [ $allocated_memory_mb -ge 2000 ]; then
		# 2GB+: Small model
		MODEL_NAME="tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
		MODEL_URL="https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
		MODEL_DESC="TinyLlama 1.1B Q4_K_M (700MB model) - Small memory config (2GB+)"
	elif [ $allocated_memory_mb -ge 1000 ]; then
		# 1GB+: Minimal model
		MODEL_NAME="tinyllama-1.1b-chat-v1.0.Q2_K.gguf"
		MODEL_URL="https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q2_K.gguf"
		MODEL_DESC="TinyLlama 1.1B Q2_K (460MB model) - Minimal memory config (1GB+)"
	else
		# <1GB: Micro model
		MODEL_NAME="tinyllama-1.1b-chat-v1.0.Q2_K.gguf"
		MODEL_URL="https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q2_K.gguf"
		MODEL_DESC="TinyLlama 1.1B Q2_K (460MB model) - Micro memory config (<1GB)"
	fi
	
	echo "Selected model: $MODEL_DESC"
}

# Validate mutually exclusive model configuration
if [ -n "$LLAMACPP_MODEL_URL" ] && [ -n "$LLAMACPP_MODEL_SIZE" ]; then
	die "LLAMACPP_MODEL_URL and LLAMACPP_MODEL_SIZE are mutually exclusive. Please specify only one."
fi

# Determine model to use
if [ -n "$LLAMACPP_MODEL_URL" ]; then
	# User specified custom model URL - extract filename as model name
	MODEL_NAME=$(basename "$LLAMACPP_MODEL_URL")
	MODEL_URL="$LLAMACPP_MODEL_URL"
	echo "Using custom model: $MODEL_NAME from $MODEL_URL"
elif [ -n "$LLAMACPP_MODEL_SIZE" ]; then
	# Handle memory expressions at runtime
	. $SHELLPACK_INCLUDE/include-sizes.sh
	getmemtotals
	ALLOCATED_BYTES=$(($LLAMACPP_MODEL_SIZE))
	select_model_by_memory_bytes $ALLOCATED_BYTES
	echo "Using model: $MODEL_NAME"
else
	die "Invalid model configuration. Either specify --model-url, or set LLAMACPP_MODEL_SIZE to a memory expression like '\$MEMTOTAL_BYTES*4/5'"
fi

MODEL_PATH="$MODEL_DIR/$MODEL_NAME"

# Download model if not present
if [ ! -f "$MODEL_PATH" ]; then
	echo "Downloading model: $MODEL_NAME"
	
	# Try huggingface-cli first if available
	if command -v huggingface-cli >/dev/null 2>&1; then
		echo "Using huggingface-cli for download"
		# Extract repository and filename from URL
		if [[ "$MODEL_URL" =~ https://huggingface.co/([^/]+/[^/]+)/.*/([^/]+)$ ]]; then
			REPO="${BASH_REMATCH[1]}"
			FILENAME="${BASH_REMATCH[2]}"
			cd "$MODEL_DIR"
			huggingface-cli download "$REPO" "$FILENAME" --local-dir . --local-dir-use-symlinks False || die "Failed to download model with huggingface-cli"
			cd "$LLAMACPP_INSTALL_PATH"
		else
			die "Could not parse HuggingFace URL: $MODEL_URL"
		fi
	else
		echo "Using wget for download"
		wget --user-agent="Mozilla/5.0" --header="Accept: application/octet-stream" -O "$MODEL_PATH" "$MODEL_URL" || die "Failed to download model with wget"
	fi
fi

cd $LLAMACPP_INSTALL_PATH

# Set up library path
export LD_LIBRARY_PATH="$LLAMACPP_INSTALL_PATH/lib:$LD_LIBRARY_PATH"

echo "Starting llama-bench with model: $MODEL_NAME, backend: $LLAMACPP_COMPUTE_BACKEND"

mmtests_activity llamacpp

###SHELLPACK threads_large_stride_begin $LLAMACPP_MIN_THREADS $LLAMACPP_MAX_THREADS
	echo "Running with $NR_THREADS/$LLAMACPP_MAX_THREADS threads"
	
	###SHELLPACK iteration_begin $LLAMACPP_ITERATIONS
		echo "Running iteration $ITERATION/$LLAMACPP_ITERATIONS with $NR_THREADS/$LLAMACPP_MAX_THREADS threads"
		
		echo 3 > /proc/sys/vm/drop_caches
		
		# Build llama-bench command arguments
		LLAMABENCH_ARGS=(
			--model "$MODEL_PATH"
			--threads "$NR_THREADS"
			--flash-attn 1
			-pg 1024,256
		)
		
		# Add optional parameters only if set
		if [ -n "$LLAMACPP_PROMPT_TOKENS" ]; then
			LLAMABENCH_ARGS+=(--n-prompt "$LLAMACPP_PROMPT_TOKENS")
		fi
		if [ -n "$LLAMACPP_GEN_TOKENS" ]; then
			LLAMABENCH_ARGS+=(--n-gen "$LLAMACPP_GEN_TOKENS")
		fi
		if [ -n "$LLAMACPP_BATCH_SIZE" ]; then
			LLAMABENCH_ARGS+=(--batch-size "$LLAMACPP_BATCH_SIZE")
		fi
		if [ -n "$LLAMACPP_MODEL_NUMA" ]; then
			LLAMABENCH_ARGS+=(--numa "$LLAMACPP_MODEL_NUMA")
		fi
		if [ "$LLAMACPP_WARMUP" = "no" ]; then
			LLAMABENCH_ARGS+=(--no-warmup)
		fi
		if [ -n "$LLAMACPP_NO_KVOFFLOAD" ]; then
			LLAMABENCH_ARGS+=(--no-kv-offload "$LLAMACPP_NO_KVOFFLOAD")
		fi

		# Configure compute backend
		# Use 999 layers to ensure all model layers run on GPU regardless of model size
		case "$LLAMACPP_COMPUTE_BACKEND" in
		cuda)
			echo "Using CUDA GPU backend"
			LLAMABENCH_ARGS+=(--n-gpu-layers 999)
			;;
		opencl)
			echo "Using OpenCL GPU backend"
			LLAMABENCH_ARGS+=(--n-gpu-layers 999)
			;;
		vulkan)
			echo "Using Vulkan GPU backend"
			LLAMABENCH_ARGS+=(--n-gpu-layers 999)
			;;
		*)
			echo "Using CPU backend"
			;;
		esac

		echo "Command: ./bin/llama-bench ${LLAMABENCH_ARGS[@]}"
		
		monitor_pre_hook $LOGDIR_RESULTS/llamacpp-${NR_THREADS}-${ITERATION}
		save_rc ./bin/llama-bench "${LLAMABENCH_ARGS[@]}" 2>&1 | tee $LOGDIR_RESULTS/llamacpp-${NR_THREADS}-${ITERATION}.log
		recover_rc
		if [ $? -ne 0 ]; then
			die "llama-bench execution failed"
		fi
		monitor_post_hook $LOGDIR_RESULTS/llamacpp-${NR_THREADS}-${ITERATION}
	###SHELLPACK iteration_end
###SHELLPACK threads_stride_end

echo llama.cpp benchmark completed successfully